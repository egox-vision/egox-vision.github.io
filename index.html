<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="neurips, workshop, egocentric vision, human experience, foundation models">

  <link rel="shortcut icon" href="imgs/favicon.png">



  <title>EgoX at NeurIPS 2025</title>
  <meta name="description" content="Website for the Workshop on EgoX at NeurIPS 2025">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="EgoX at NeurIPS 2025">
  <meta property="og:url" content="https://egox.github.io/">
  <meta property="og:description" content="Website for the Workshop on EgoX at NeurIPS 2025">
  <meta property="og:site_name" content="EgoX at NeurIPS 2025">
  <meta property="og:image" content="">
  <meta property="og:image:url" content="">

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="EgoX at NeurIPS 2025">
  <!-- <meta name="twitter:image" content="https://3dv-in-ecommerce.github.io/static/img/bg.png"> -->
  <meta name="twitter:url" content="https://3dv-in-ecommerce.github.io/">
  <meta name="twitter:description" content="Website for the Workshop on EgoX at NeurIPS 2025">

  <!-- CSS  -->
  <link rel="stylesheet" type="text/css" href="./bootstrap.min.css">
  <link rel="stylesheet" type="text/css" href="./main.css" media="screen,projection">
<link href="chrome-extension://idpbkophnbfijcnlffdmmppgnncgappc/css/content.css" rel="stylesheet" type="text/css"></head>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="https://3dv-in-ecommerce.github.io/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Challenge</a></li>
        <li><a href="#dates">Important Dates</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <!--<li><a href="#accepted">Accepted Papers</a></li>-->
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>EgoX: Building the Foundation Models of Human Experience</h1></center>
    <center><h2>NeurIPS 2025 Workshop</h2></center>
    <!-- <center><span style="font-weight:400;">October 2, 2023 @ Paris, France</span></center> -->
    <!-- <center><span style="font-weight:400;">Room E06, Paris Convention Center</span></center> -->
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br>
  </div>
</div>

<hr>

<!--<b>游닉 Calling all researchers and enthusiasts! 游 Join our thrilling fine-grained 3D part labeling challenge built on the Amazon Berkeley Objects (ABO) Dataset: <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>.</b>-->
<!-- <p><b>游닉 Live Q&amp;A on Slido: <a href="https://app.sli.do/event/iYUGKVgj6AVdjGhY5dzvAd" target="_blank">https://tinyurl.com/3DVeComm-slido</a> (use it to ask questions for presentations and panel discussion).</b> <br>
<b>游닉 Remote presentations on Zoom: <a href="https://sfu.zoom.us/j/82710628380?pwd=OQaJJGWXuRr7IYakyPd8k6Em6iUAeg.1" target="_blank">https://tinyurl.com/3DVeComm-zoom</a>.</b></p> -->

<!-- <b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECCV2022/View_agenda/236653">here</a> (ECCV registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtcVkWg2n8">https://forms.gle/FFFVHVeTtcVkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


 <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      This workshop aims to bring together researchers working on ...
      <!-- This workshop aims to bring together researchers working on 3D computer vision and graphics for eCommerce, with a focus on the three topics: (1) 3D object/scene modeling and understanding in 3D eCommerce such as semantic segmentation, affordance and motion, multi-view reconstruction; (2) human modeling and fashion in 3D eCommerce such as virtual try-ons and personalized fashion recommendation, and (3) language-assisted reasoning such as shape/scene synthesis from texts and language grounding in 3D models. We invited 6 keynote speakers from academia and 3 talks from industry experts. We will also host a challenge on 3D part labeling for 3D models from real products sold online. -->
     <!--This workshop aims to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.-->
    </p>
  </div>
</div>
<p><br></p>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <!-- <p>All times in Paris Time (UTC+02:00)</p> -->
     <p>TBD</p>
  </div>
</div>
<p><br></p>
<!-- <div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>8:50am - 9:00am</td>
          <td>Welcome and Introduction (Richard Zhang)</td>
          <td></td>
        </tr>
        <tr>
          <td>9:00am - 9:35am</td>
          <td>
          Invited Talk 1 (Leonidas Guibas)
          <br>
          <i>Title: Compositional Object Modeling: Parts, Language, and Functionality</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>9:35am - 10:10am</td>
          <td>
          Invited Talk 2 (Rana Hanocka)
          <br>
          <i>Title: Data-Driven Shape Editing without 3D Data</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:10am - 10:20am</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>10:20am - 10:55am</td>
          <td>
          Invited Talk 3 (Michael Black)
          <br>
          <i>Title: Implicit, Explicit, Real, and Synthetic: Spinning the Virtual Fashion Flywheel</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>10:55am - 11:30am</td>
          <td>Invited Talk 4 (Ming Lin)
          <br>
          <i>Title: Dynamics-Inspired Learning-based Virtual Try-On</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>11:30am - 12:00pm</td>
          <td>Invited student paper presentations
          <br>
          <i>HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling</i> (Fenggen Yu) <br>
          <i>TriCoLo: Trimodal Contrastive Loss for fine-grained Text to Shape Retrieval</i> (Angel Chang) <br>
          <i>Improving Unsupervised Visual Program Inference with Code Rewriting Families</i> (Aditya Ganeshan) <br>
          <i>Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior</i> (Junshu Tang) <br>
          <i>D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field</i> (Xueting Yang)
          </td>
          <td></td>
        </tr>
        <tr>
          <td>12:00pm - 1:30pm</td>
          <td>Lunch break</td>
          <td></td>
        </tr>
        <tr>
          <td>1:30pm - 2:00pm</td>
          <td>Winner presentations of the challenge</td>
          <td></td>
        </tr>
        <tr>
          <td>2:00pm - 2:35pm</td>
          <td>
          Invited Talk 5 (Roozbeh Mottaghi)
          <br>
          <i>Title: Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>2:35pm - 3:10pm</td>
          <td>
          Invited Talk 6 (Katerina Fragkiadaki)
          <br>
          <i>Title: 3D Part Segmentation and Reconstruction with Little or No Training</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>3:10pm - 3:20pm</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>3:20pm - 3:45pm</td>
          <td>
          Industry Talk 1 (Eric Bennett from Amazon)
          <br>
          <i>Title: Scaling 3D eCommerce Innovations <br> A 10,000 Meter View on Building Durable 3D Content Creation Algorithms for Growth</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>3:45pm - 4:10pm</td>
          <td>
          Industry Talk 2 (Itamar Berger from Snap)
          <br>
          <i>Title: Snap to Fit: AR Try-on in Snapchat</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>4:10pm - 4:35pm</td>
          <td>
          Industry Talk 3 (Chengfei Lv from Alibaba)
          <br>
          <i>Title: Introduction to 3D and XR Technology in Taobao</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>4:35pm - 5:00pm</td>
          <td>Panel discussion and community building <br>
          <i>Panelists: Ming Lin, Rana Hanocka, Michael Black</i><br>
          <i>Moderator: Daniel Ritchie</i></td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
-->
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
    <p>TBD</p>
  </div>
</div>
<p><br></p>

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://dimadamen.github.io/about.html"><img class="people-pic" style="float:left;margin-right:50px;" src="./imgs/dima.jpg"></a>
    <p>
      <b><a href="https://dimadamen.github.io/about.html">Dima Damen</a></b> Professor at the University of Bristol and Senior Research Scientist at Google DeepMind. She is best known for her leading works in Egocentric Vision, and has also contributed to novel research questions including mono-to-3D, video object segmentation, assessing action completion, domain adaptation, skill/expertise determination from video sequences, discovering task-relevant objects, dual-domain and dual-time learning as well as multi-modal fusion using vision, audio and language.
    </p>
  </div>
</div>
<p><br></p>  -->

<!-- <div class="row">
  <div class="col-md-12">
    <a href="https://people.cs.uchicago.edu/~ranahanocka/"><img class="people-pic" style="float:left;margin-right:50px;" src="./egor1-worksho_files/rana.jpg"></a>
    <p>
      <b><a href="https://people.cs.uchicago.edu/~ranahanocka/">Name</a></b> Bio.
    </p>
  </div>
</div>
<p><br></p> -->

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Challenge</h2>
    <p><b>Call for particpations: </b>The workshop will host a competition focused on ...</p>
    <br>
    <p><b>Submission site: </b>TBD</p>
  </div>
</div>
<p><br></p>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Release of train sets</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Release of test set</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Submission deadline</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td>TBD</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
<p><br></p>
<!--<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0001-poster.png" width='700'><br/>
    <a href=''>3D GAN Inversion for Controllable Portrait Image Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0002-poster.png" width='700'><br/>
    <a href=''>3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0003-poster.png" width='700'><br/>
    <a href=''>3D Semantic Label Transfer and Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, Benj치min Baffy, G치bor Baranyi, Joul Skaf, L치szl칩 Kop치csi, Daniel Sonntag, G치bor S칬r칬s, and Andr치s L콈rincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0004-poster.png" width='700'><br/>
    <a href=''>Generative Multiplane Images: Making a 2D GAN 3D-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David G칲era, Zhile Ren, Alexander G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0005-poster.png" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah L칛hner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0006-poster.png" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKCVA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0007-poster.png" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0008-poster.png" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0009-poster.png" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0010-poster.png" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexander Keller, Sameh Khamis, Thomas M칲eller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0011-poster.png" width='700'><br/>
    <a href=''>Recovering Detail in 3D Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0012-poster.png" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0013-poster.png" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>
-->

<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row text-center">

  <div class="col-xs-2">
    <a href="https://shulin16.github.io/">
      <img class="people-pic" src="https://shulin16.github.io/images/Shulin_2025.jpg">
    </a>
    <div class="people-name">
      <a href="https://shulin16.github.io/">Shulin Tian</a>
      <h6>Nanyang Technological University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://suikei-wang.github.io/">
      <img class="people-pic" src="imgs/ruiqi.jpg">
    </a>
    <div class="people-name">
      <a href="https://suikei-wang.github.io/">Ruiqi Wang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://jingkang50.github.io">
      <img class="people-pic" src="https://jingkang50.github.io/images/profile.png">
    </a>
    <div class="people-name">
      <a href="https://jingkang50.github.io">Jingkang Yang</a>
      <h6>Nanyang Technological University</h6>
    </div>
  </div>

</div>

<!-- <div class="row text-center">

  <div class="col-xs-2">
    <a href="https://egundogdu.github.io/">
      <img class="people-pic" src="./egor1-worksho_files/erhan.jpeg">
    </a>
    <div class="people-name">
      <a href="https://egundogdu.github.io/">Erhan Gundogdu</a>
      <h6>Amazon</h6>
    </div>
  </div>



  </div>

  

  

</div> -->

<hr>

<h2>Senior Organizers</h2>
<div class="row text-center">
  <div class="col-xs-2">
    <a href="https://liuziwei7.github.io/">
      <img class="people-pic" src="imgs/ziwei.png">
    </a>
    <div class="people-name">
      <a href="https://liuziwei7.github.io/">Ziwei Liu</a>
      <h6>Nanyang Technological University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dimadamen.github.io/about.html">
      <img class="people-pic" src="imgs/dima.jpg">
    </a>
    <div class="people-name">
      <a href="https://dimadamen.github.io/about.html">Dima Damen</a>
      <h6>University of Bristol</h6>
    </div>
  </div>


  <!--
  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="./egor1-worksho_files/hao.jpg">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University &amp; Amazon</h6>
    </div>
  </div> -->
</div>

<p><br>
<br></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
       We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://3dv-in-ecommerce.github.io/">3dv-in-ecommerce</a></span> for the webpage format.
    </p>
  </div>
</div>

<p><br></p>


      </div>
    </div>

    

    <script type="text/javascript" src="./jquery.min.js"></script>
    <script type="text/javascript" src="./bootstrap.min.js"></script>
    <script type="text/javascript" src="./main.js"></script>
  

</body><div style="all: initial !important;"><template shadowrootmode="open"><link rel="stylesheet" type="text/css" href="chrome-extension://idpbkophnbfijcnlffdmmppgnncgappc/css/content.css"></template></div></html>
